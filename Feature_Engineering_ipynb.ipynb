{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature Engineering #####"
      ],
      "metadata": {
        "id": "7cNcvvvMshYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is a parameter?\n",
        "==>a set of facts or a fixed limit that establishes or limits how something can or must happen or be done.\n",
        "\n",
        "Q2.What is correlation? What does negative correlation mean?\n",
        "==>In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.\n",
        "A negative correlation means that two variables move in opposite directions. As one variable increases, the other tends to decrease, and vice versa. This is sometimes referred to as an inverse correlation.\n",
        "\n",
        "Q3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "==>Machine learning is a field of artificial intelligence (AI) that focuses on enabling computers to learn from data and improve their performance without explicit programming. It's about creating algorithms that can find patterns and make predictions based on the data they've been trained on. The main components of a machine learning system include data, algorithms, and evaluation metrics.\n",
        "\n",
        "Q4.How does loss value help in determining whether the model is good or not?\n",
        "==>A model's loss value is a crucial indicator of its performance, with lower values generally suggesting a better model. The loss function quantifies the difference between a model's predictions and the actual values, so a lower loss indicates a model is making more accurate predictions.\n",
        "\n",
        "Q5.What are continuous and categorical variables?\n",
        "==>In data analysis, variables are classified as either continuous or categorical. Continuous variables can take on any value within a given range, while categorical variables represent distinct groups or categories.\n",
        "\n",
        "Q6.How do we handle categorical variables in Machine Learning? What are the common t echniques?\n",
        "==>Categorical variables in machine learning are handled by converting them into a numerical format that algorithms can understand. Common techniques include one-hot encoding, label encoding, ordinal encoding, binary encoding, and target encoding. One-hot encoding creates binary columns for each category, while label encoding assigns unique integers. Ordinal encoding preserves the order of categories, and binary encoding uses binary digits. Target encoding replaces categories with the average target value for that category.\n",
        "Common Techniques for Handling Categorical Variables:\n",
        "* One-Hot Encoding\n",
        "* Label Encoding\n",
        "* Ordinal Encoding\n",
        "* Binary Encoding\n",
        "* Target Encoding\n",
        "* Frequency Encoding\n",
        "\n",
        "Q7.What do you mean by training and testing a dataset?\n",
        "==>In machine learning, \"training\" and \"testing\" a dataset refers to the process of splitting your data into two sets: a training set and a testing set. The training set is used to train your model, meaning it's the data your model learns from to build its predictive capabilities. The testing set, on the other hand, is used to evaluate how well your trained model performs on new, unseen data. This helps you assess the model's accuracy and generalization ability.\n",
        "\n",
        "Q8.What is sklearn.preprocessing?\n",
        "==>The sklearn.preprocessing module in scikit-learn provides a suite of functions and classes designed to transform raw data into a format more suitable for machine learning algorithms. It encompasses various techniques for data scaling, normalization, encoding, and imputation. Preprocessing steps are crucial for improving model performance and ensuring compatibility with different algorithms.\n",
        "\n",
        "Q9.What is a Test set?\n",
        "==>In machine learning and data science, a test set is a portion of the dataset used to evaluate the performance of a model that has been trained on a separate dataset, called the training set.\n",
        "\n",
        "Q10.How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "==>All About Train Test Split - Shiksha OnlineIn Python, data is typically split for model fitting (training and testing) using the train_test_split function from sklearn.model_selection. This function divides a dataset into two subsets: one for training the model and the other for evaluating its performance. A typical approach to a machine learning problem involves data collection, preparation, model selection, training, evaluation, and potentially parameter tuning.\n",
        "Approaching a Machine Learning Problem:\n",
        "1.A typical machine learning workflow involves several key steps:\n",
        "2.Problem Definition: Clearly define the problem you want to solve and the goals of your model.\n",
        "3.Data Collection: Collect relevant data from various sources.\n",
        "Data Preparation: Clean, preprocess, and transform the data to make it suitable for model training. This may include handling missing values, scaling features, and encoding categorical variables.\n",
        "4.Model Selection: Choose an appropriate machine learning model based on the problem type (classification, regression, clustering, etc.) and the characteristics of your data.\n",
        "5.Model Training: Train your chosen model on the training data.\n",
        "6.Model Evaluation: Evaluate the performance of your trained model on the testing data. This can involve using various metrics to assess the model's accuracy, precision, recall, and other relevant aspects.\n",
        "7.Hyperparameter Tuning: Optimize the model's hyperparameters to improve its performance.\n",
        "8.Model Deployment: Deploy the trained model for use in a real-world application.\n",
        "\n",
        "Q11.Why do we have to perform EDA before fitting a model to the data?\n",
        "==>Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons. It allows you to understand the data's characteristics, identify patterns, and detect potential problems, ultimately leading to better model selection, feature engineering, and evaluation. Without EDA, you'd be building a model based on assumptions, potentially missing important insights or errors in the data that could negatively impact its performance.\n",
        "\n",
        "Q12.What is correlation?\n",
        "==>In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.\n",
        "\n",
        "Q13.What does negative correlation mean?\n",
        "==>A negative correlation means that two variables move in opposite directions. As one variable increases, the other tends to decrease, and vice versa. This is sometimes referred to as an inverse correlation.\n",
        "\n",
        "Q14.How can you find correlation between variables in Python?\n",
        "==>To find the correlation between variables in Python, several libraries can be used, each offering different methods and functionalities:\n",
        "1. Pandas\n",
        "Pandas is a powerful library for data manipulation and analysis, and it provides the corr() method to calculate the correlation matrix between columns in a DataFrame. By default, it calculates the Pearson correlation coefficient, which measures the linear relationship between two continuous variables.\n",
        "2. NumPy\n",
        "NumPy is a fundamental library for numerical computing in Python, and it includes the corrcoef() function to calculate the Pearson correlation coefficient between two arrays.\n",
        "3. SciPy\n",
        "SciPy is a library for scientific and technical computing, and it offers functions like pearsonr(), spearmanr(), and kendalltau() in the scipy.stats module to calculate different types of correlation coefficients.\n",
        "4. Seaborn and Matplotlib\n",
        "Seaborn and Matplotlib are libraries for data visualization, and they can be used to create scatter plots and heatmaps to visualize the relationships between variables. Scatter plots display individual data points and can help identify correlations or clusters, while heatmaps provide a visual representation of the correlation matrix.\n",
        "\n",
        "Q15.What is causation? Explain difference between correlation and causation with an example.\n",
        "==>Causation means one event directly causes another to occur, while correlation simply indicates a relationship or association between two events. In other words, correlation doesn't necessarily imply causation; two things may happen together but one doesn't cause the other.\n",
        "Example:\n",
        "1.Correlation:\n",
        "Eating ice cream and getting sunburned. There is a correlation between these events as they both tend to happen on sunny days. However, neither event causes the other. The primary cause of both is exposure to sunlight.\n",
        "2.Causation:\n",
        "A broken window. The act of throwing a ball causes the window to break. The ball hitting the window directly leads to the window shattering, establishing a clear cause-and-effect relationship.\n",
        "\n",
        "Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "==>An optimizer is an algorithm used to adjust parameters (like weights and biases) in a model during training to minimize a loss function. It's like a tool that helps a model learn efficiently and accurately. Different types of optimizers use various strategies to find the best parameter values.\n",
        "Types of Optimizers and Examples:\n",
        "1. Gradient Descent:\n",
        "What it does:\n",
        "Iteratively adjusts parameters in the direction of the steepest descent of the loss function.\n",
        "Example:\n",
        "Imagine a ball rolling down a hill. The ball is the model, and the hill is the loss function. Gradient descent uses the slope of the hill (gradient) to guide the ball (model) towards the bottom (minimum loss).\n",
        "2. Stochastic Gradient Descent (SGD):\n",
        "What it does:\n",
        "Similar to gradient descent, but updates parameters based on a single training example (or a small batch) at a time.\n",
        "Example:\n",
        "Instead of using the entire dataset to calculate the gradient (like in batch gradient descent), SGD updates the model's weights after processing each data point. This can lead to faster training, especially with large datasets.\n",
        "3. Momentum:\n",
        "What it does:\n",
        "Adds a momentum term to the gradient update, similar to how a ball gains momentum as it rolls downhill. This helps the model to move more smoothly and efficiently towards the minimum.\n",
        "Example:\n",
        "Imagine the ball rolling down a hill. Instead of stopping at every small dip, it continues to roll, gaining momentum and accelerating towards the bottom.\n",
        "4. AdaGrad (Adaptive Gradient):\n",
        "What it does:\n",
        "Adapts the learning rate for each parameter based on how often it's updated. Parameters that are updated more frequently have a smaller learning rate, and those updated less frequently have a larger learning rate.\n",
        "Example:\n",
        "Imagine the ball rolling down a hill. If it encounters a wide, smooth area, it can move faster. If it encounters a steep, narrow area, it needs to move more carefully to avoid overshooting.\n",
        "5. Adam (Adaptive Moment Estimation):\n",
        "What it does:\n",
        "Combines the advantages of momentum and AdaGrad, using both the first and second moments of the gradients to estimate the learning rate for each parameter.\n",
        "Example:\n",
        "It's like having both a ball rolling down a hill with momentum (like in Momentum) and a ball that adapts its speed based on the slope (like in AdaGrad).\n",
        "\n",
        "Q17.What is sklearn.linear_model ?\n",
        "==>sklearn.linear_model is a module in the scikit-learn (sklearn) library in Python that implements various linear models for regression and classification tasks. Linear models predict the target variable as a linear combination of the input features. It includes algorithms like.\n",
        "\n",
        "Q18.What does model.fit() do? What arguments must be given?\n",
        "==>The model.fit() function in machine learning frameworks like TensorFlow and Keras is used to train a model on a given dataset. It adjusts the model's internal parameters (weights and biases) to minimize the loss function, effectively learning the patterns and relationships within the data.\n",
        "The basic required arguments for model.fit() are:\n",
        "x:\n",
        "Training data. It can be a NumPy array, a list of arrays (for models with multiple inputs), or a dataset object.\n",
        "y:\n",
        "Target data (labels). It should correspond to the training data and can also be a NumPy array or a list.\n",
        "\n",
        "Q19.What does model.predict() do? What arguments must be given?\n",
        "==>Purpose : model. predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics.\n",
        "\n",
        "Q20.What are continuous and categorical variables?\n",
        "==>In data analysis, variables are classified as either continuous or categorical. Continuous variables can take on any value within a given range, while categorical variables represent distinct groups or categories.\n",
        "\n",
        "Q21.What is feature scaling? How does it help in Machine Learning?\n",
        "==>Feature scaling in machine learning is the process of transforming numerical features in a dataset to a common scale or range. It helps ensure that all features contribute equally to the model and prevents features with larger values from dominating the learning process.\n",
        "Example:\n",
        "Imagine a dataset with features like \"age\" (ranging from 0 to 100) and \"income\" (ranging from 0 to millions). Without scaling, the \"income\" feature might dominate the model, as its values are much larger than \"age\". By scaling both features, they become comparable, allowing the model to learn from all features equally.\n",
        "\n",
        "Q22.How do we perform scaling in Python?\n",
        "==>Scaling data in Python involves transforming numerical features to a similar range. This ensures that no single feature dominates the others during model training, improving the performance of machine learning algorithms. Several scaling techniques can be implemented using the scikit-learn library.\n",
        "\n",
        "Q23.What is sklearn.preprocessing?\n",
        "==>sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to preprocess data before training machine learning models. Preprocessing steps are crucial for improving the performance and accuracy of machine learning models. The module includes various techniques for data scaling, normalization, encoding, and transformation.\n",
        "\n",
        "Q24.How do we split data for model fitting (training and testing) in Python?\n",
        "==>The training set data must be large enough to capture variability in the data but not so large that the model overfits the training data. The optimal split ratio depends on various factors. The rough standard for train-validation-test splits is 60-80% training data, 10-20% validation data, and 10-20% test data.\n",
        "\n",
        "Q25.Explain data encoding?\n",
        "==>Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing. It involves transforming information into a specific code or format, ensuring it can be read and interpreted by a computer or other system. This process is crucial for various applications, including ensuring data integrity, security, and compatibility between different systems."
      ],
      "metadata": {
        "id": "J1qvq6DDsnS-"
      }
    }
  ]
}